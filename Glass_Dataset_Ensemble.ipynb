{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Glass Dataset - Ensemble.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silveiraluiza/machine-learning/blob/main/Glass_Dataset_Ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X095JUDe0TqB"
      },
      "source": [
        "**Missão 7:**\n",
        "\n",
        "**Equipe:**\n",
        "- Luiza Carvalho Silveira (lcs11@cin.ufpe.br)\n",
        "- Andersson Alves da Silva (aas9@cin.ufpe.br)\n",
        "- Emerson Lima do Nascimento (eln3@cin.ufpe.br)\n",
        "\n",
        "# **Ensemble - Realizar a implementação do bagging, boosting e stacking para a base de dados do Glass.**\n",
        "\n",
        "- Realizar experimento com a implementação de comitês bagging, boosting e stacking para base de dados do Glass. Os modelos base para formação do comitê poderão ser KNN, SVM, Árvore de Decisão e/ou Redes Neurais Artificiais. Realizar uma busca de tal modo que os comitês apresentem desempenho superior ao desempenho solo dos classificadores. Comparar o desempenho com os modelos de SVM desenvolvidos na última missão. Apresentar análise, os principais desafios encontrados para a construção dos comitês e conclusões."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhhrfnUR0Mqc"
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_1K2k2a2aIr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "70470aa8-dec8-4bcd-e566-43303d895c31"
      },
      "source": [
        "accur_end_stacking = np.array([])\n",
        "accur_end_boosting = np.array([])\n",
        "accur_end_bagging = np.array([])\n",
        "accur_end_knn = np.array([])\n",
        "accur_end_svm = np.array([])\n",
        "\n",
        "dataset = np.array(pd.read_csv('drive/My Drive/data/glass.csv', sep=','))  # Importando os dados\n",
        "target = dataset[:,-1]; # Alvos\n",
        "features = dataset[:,:-1] # Caracteristicas\n",
        "print(target)\n",
        "print()\n",
        "print(features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 5. 5. 5. 5. 5.\n",
            " 5. 5. 5. 5. 5. 5. 5. 5. 6. 6. 6. 6. 6. 6. 6. 6. 6. 7. 7. 7. 7. 7. 7. 7.\n",
            " 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7.]\n",
            "\n",
            "[[ 1.52101 13.64     4.49    ...  8.75     0.       0.     ]\n",
            " [ 1.51761 13.89     3.6     ...  7.83     0.       0.     ]\n",
            " [ 1.51618 13.53     3.55    ...  7.78     0.       0.     ]\n",
            " ...\n",
            " [ 1.52065 14.36     0.      ...  8.44     1.64     0.     ]\n",
            " [ 1.51651 14.38     0.      ...  8.48     1.57     0.     ]\n",
            " [ 1.51711 14.23     0.      ...  8.62     1.67     0.     ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96CEWXEG4NjW"
      },
      "source": [
        "Execultando o bagging 100x para encontrar a acurácia média"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fcjTEjN2xbS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "5cb9b0c8-b5a0-42c7-dee4-8ceb257a248d"
      },
      "source": [
        "for i in np.arange(100):\n",
        "    \n",
        "    # Separando entre dados de treino e teste (10% dos dados para teste)\n",
        "    x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.10)\n",
        "    \n",
        "    # Normalizando dados através do método de 'Padronização'\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(x_train)\n",
        "    x_train = scaler.transform(x_train)\n",
        "    x_test = scaler.transform(x_test)\n",
        "    \n",
        "    # Chamando o Stacking com o modelo do K-Viznhos Mais Próximos (KNN)\n",
        "    estimator = [('rf', RandomForestClassifier(n_estimators=10, random_state=42))]\n",
        "    stacking = StackingClassifier(estimators = estimator, final_estimator = LogisticRegression())\n",
        "\n",
        "    # Chamando o Boosting pelo método do AdaBoost\n",
        "    boosting = AdaBoostClassifier(n_estimators=10) \n",
        "\n",
        "    # Chamando o classificador bagging com o KNN\n",
        "    bagging = BaggingClassifier(base_estimator = KNeighborsClassifier(n_neighbors=3), n_estimators=10, random_state=0)\n",
        "    \n",
        "    # Chamando apenas o KNN com o número de vizinhos sendo igual a 5\n",
        "    knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "    # Chamando apenas o SVM com \n",
        "    svm_linear = svm.SVC(kernel='linear', C=1.0)\n",
        "    \n",
        "    # Treinando o stacking\n",
        "    stacking.fit(x_train, y_train)    \n",
        "    # Treinando o boosting\n",
        "    boosting.fit(x_train, y_train)\n",
        "    # Treinando o bagging\n",
        "    bagging.fit(x_train, y_train)\n",
        "    # Treinando o KNN \n",
        "    knn.fit(x_train, y_train)\n",
        "    # Treinando o SVM \n",
        "    svm_linear.fit(x_train, y_train)\n",
        "\n",
        "    # Testando classificador\n",
        "    y_pred_stacking = stacking.predict(x_test)\n",
        "    y_pred_boosting = boosting.predict(x_test)\n",
        "    y_pred_bagging = bagging.predict(x_test)\n",
        "    y_pred_knn = knn.predict(x_test)\n",
        "    y_pred_svm = svm_linear.predict(x_test)\n",
        "\n",
        "    # Computando a Performance do Stacking \n",
        "    accur_stacking = 100*np.sum(np.array(y_pred_stacking == y_test))/len(y_pred_stacking)\n",
        "    accur_end_stacking = np.append(accur_end_stacking, accur_stacking)\n",
        "\n",
        "    # Computando a Performance do Boosting \n",
        "    accur_boosting = 100*np.sum(np.array(y_pred_boosting == y_test))/len(y_pred_boosting)\n",
        "    accur_end_boosting = np.append(accur_end_boosting, accur_boosting)\n",
        "    \n",
        "    # Computando a Performance do Bagging \n",
        "    accur_bagging = 100*np.sum(np.array(y_pred_bagging == y_test))/len(y_pred_bagging)\n",
        "    accur_end_bagging = np.append(accur_end_bagging, accur_bagging)\n",
        "    \n",
        "    # Computando a Performance do KNN \n",
        "    accur_knn = 100*np.sum(np.array(y_pred_knn == y_test))/len(y_pred_knn)\n",
        "    accur_end_knn = np.append(accur_end_knn, accur_knn)\n",
        "\n",
        "    # Computando a Performance do KNN \n",
        "    accur_svm = 100*np.sum(np.array(y_pred_svm == y_test))/len(y_pred_svm)\n",
        "    accur_end_svm = np.append(accur_end_svm, accur_svm)\n",
        "\n",
        "print('Acerto médio STACKING:', np.mean(accur_end_stacking))    \n",
        "print('Acerto médio BOOSTING:', np.mean(accur_end_boosting))    \n",
        "print('Acerto médio BAGGING:', np.mean(accur_end_bagging))\n",
        "print('Acerto médio KNN:', np.mean(accur_end_knn))\n",
        "print('Acerto médio SVM:', np.mean(accur_end_svm))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acerto médio STACKING: 73.04545454545453\n",
            "Acerto médio BOOSTING: 41.95454545454544\n",
            "Acerto médio BAGGING: 67.63636363636365\n",
            "Acerto médio KNN: 63.36363636363638\n",
            "Acerto médio SVM: 61.49999999999999\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}